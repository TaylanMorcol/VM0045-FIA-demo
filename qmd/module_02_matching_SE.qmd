---
title: "MODULE 02 - Matching FIA plots to sample unit"
date: 2025-11-08
author: 
  - name: Taylan Morcol
    google_scholar: https://scholar.google.com/citations?user=OAvldLMAAAAJ&hl=en
    website: https://morcol.com 
    linkedin: https://www.linkedin.com/in/taylan-morcol
format: html
output-file: module_02_matching_SE.html
---



MODULE UNDER CONSTRUCTION...





[‚Üê Back to module index](index.html)

**Disclaimer:** This material was prepared on my own personal time and is offered in my personal capacity only. It does not represent the views of, nor is it endorsed by, any current or past employer.

# ---SUMMARY---
In the last module, working with a small set of FIA plots from Rhode Island, we saw how to apply some of the VM0045 donor pool selection criteria. In this module, we turn our attention to the Southeastern US. Targetting a larger geographic area this time, we apply the full set of donor selection criteria to FIA plots in loblolly pine forests in ecological section 232J (i.e., Southern Atlantic Coastal Plains and Flatwoods section of the Outer Coastal Plain Mixed Forest province). We then select an FIA plot to serve as a _pseudo_-sample unit and demonstrate the composite baseline matching procedure. 

![Ecological section 232J (pink outline) interects four US states (northeast to southwest): North Carolina, South Carolina (blue outline), Georgia (blue outline), and Florida. Map excerpt from Cleland et al., (2007), https://doi.org/10.2737/WO-GTR-76D. Outlines added for clarity.](/images/Cleland_etAl_2007_section232J.png)

As seen in the map, section 232J intersects 4 states, each of which is substantially larger than Rhode Island. Thus, this module deals with much larger data files than the first module. As we'll see, this requires some modifications to how we download, read in, and process the FIA data.

In a future module, I'll demonstrate the matching procedure for multiple sample units, constructing a composite baseline for each one. With multiple samples, one can then assess match quality, another necessary aspect of VM0045. Thus, this module and the next approximate how one might approach generating a composite baseline for an IFM project involving loblolly pine plantings.

# ---R PIPELINE---
## Load packages
In addition to the packages used in Module 01, this module also uses:

- `here`, which is used to find current working directory

- `parallel`, which is used to determine the number of cores available for parallel processing.

- `callr`, which allows us to run isolated, one-off sessions of R during data loading and carbon calculations. This helps with memory management issues that come with dealing with large data files.

- `geosphere`, which is used for calculating geographic distance, one of the covariates involved in the matching procedure.

- `optmatch`, which is used for calculating Mahalanobis distances for the baseline matching procedure.

```{r, message=FALSE}
# packages from Module 01
library(rFIA)
library(dplyr)
library(tidyr)
library(ggplot2)
library(maps)

# packages new to this module
library(parallel)
library(callr)
library(optmatch)
library(geosphere)
library(here)
```

## Download data from FIADB
This is where things start to get tricky. In Module 01, I purposefully selected a small geographic area (Rhode Island) to keep things simple and avoid having to deal with memory management. But in this module, we're dealing with a larger dataset. My personal laptop running Ubuntu 20.04 has only 12 GB available memory available, so to get the script to run without crashing, I had to employ some different tactics.

The first difference is that the combined FIADB files download for North Carolina, South Carolina, Georgia, and Florida is a much larger size than Rhode Island, so much so that loading the data directly into RAM via `rFIA::getFIA()` becomes impractical on a personal computer. 

I employed two strategies to deal with this.

1. Specify `getFIA()` to only download the tables required by `rFIA::carbon()`, a function used later to calculate carbon/acre. Info on required tables is found in the [function definition for `rFIA::carbonStarter`](https://github.com/doserjef/rFIA/blob/master/R/carbonStarter.R), an internal function that is called by `carbon()`. 

2. Specify `getFIA()` to download files to disk rather than loading into RAM.

```{r}
#| label: download_FIA_data
#| echo: true
#| eval: false

# 2-letter code for each desired US state
SE_states  <- c("NC","SC","GA","FL")

# minimal tables required for carbon calcs
carb_tabs <- c("PLOT", "TREE", "COND", "POP_PLOT_STRATUM_ASSGN", 
               "POP_ESTN_UNIT", "POP_EVAL", "POP_STRATUM", 
               "POP_EVAL_TYP", "POP_EVAL_GRP", "PLOTGEOM")

# file path to directory where raw FIADB csv files will be downloaded
dl_dir <- paste0(here(), "/data/raw/")

# set timeout to 1 hour, otherwise R will abort longer downloads
options(timeout=3600)
 
# get time just before downloads start (for benchmarking)
t0 <- Sys.time()

# download FIADB files
FIADB <- getFIA(states = SE_states, # specify which states' data to download
                tables = carb_tabs, # specifiy which tables to download
                load = FALSE,       # don't load data into RAM...
                dir = dl_dir)       # ... instead, download to specified directory

# get time after downloads finish (for benchmarking)
t1 <- Sys.time() 

# calculate the time it took to run the downloads
t1 - t0
```

The downloaded csv files total about 2.8 GB, and the download took about 5.3 minutes on my laptop. Right afterwards, I ran a speed test at [speedtest.net](https://www.speedtest.net). My download speed during the test was about 10x faster than while downloading the FIADB files, suggesting that download speeds were limited by the FIA server.

## Carbon/acre calculations
Next, we use functions to calculate carbon/acre for each plot visit in our dataset. Again, this is made more complicated by the larger file sizes. Even though the size of the final table of carbon values returned by `carbon()` is farily manageable for RAM, the process of doing the calculations ties up considerably more RAM to complete. And on my computer, R does not release that RAM once it's finished with the calculations, not even after running `gc()`.

To overcome this, we use `callr::r`. This function (1) initializes a new, one-off session of R, (2) runs processes within that session, (3) passes results back to the main session, and (4) terminates the one-off session. When the one-off session terminates, it releases all of its RAM back to the operating system.

We also make use of parallel processing with `carbon()` to speed up the calculations.

```{r}
#| label: carbon_calculations_parallel
#| echo: true
#| eval: false

# find total number of cores on computer
total_cores <- detectCores()

# determine number of cores to use for calculations 
cores <- case_when(total_cores > 1 ~ total_cores - 1, # if multiple cores, use 1 less than total
                   TRUE ~ 1)                          # otherwise, use 1 core

# timestamp at START of carbon calculations
t0 <- Sys.time()

cc <- callr::r(
  function(dl_dir, cores) {
    library(rFIA) 
    #
    dbp <- readFIA(dir = dl_dir,
                   inMemory = FALSE)
    carbon(dbp, 
           byPlot = TRUE, 
           byPool = TRUE, 
           nCores = cores)
  },
  args = list(dl_dir = dl_dir, cores = cores)
)

# timestamp at END of carbon calculations
t1 <- Sys.time()

# time difference using PARALLEL processing
t1 - t0

print(object.size(cc), units = "MB") # memory usage in MB
```

The carbon calculations above took about 30 seconds on my laptop, using 7 cores. The one-off R session was using up to 6 GB of RAM for the calculations, but the table of carbon calculations that it returned was only 14 MB in size. I had to close all non-essential processes on my laptop to free up enough RAM. And if those 6 GB RAM were not returned back to the OS afterwards, I would not have enough memory to continue the pipeline. That's the advantage of using `callr::r`.

Next, for comparison, I ran the same call but using only 1 core (see below). It completed in 42 seconds, vs 30 seconds when using 7 cores. Not too much of a difference.

```{r}
#| label: carbon_calculations_serial
#| echo: true
#| eval: false

# timestamp at START of carbon calculations
t0 <- Sys.time()

ccs <- callr::r(
  function(dl_dir) {
    library(rFIA) 
    #
    dbp <- readFIA(dir = dl_dir,
                   inMemory = FALSE)
    carbon(dbp, 
           byPlot = TRUE, 
           byPool = TRUE, 
           nCores = 1)
  },
  args = list(dl_dir = dl_dir)
)

# timestamp at END of carbon calculations
t1 <- Sys.time()

# time difference using SERIAL processing
t1 - t0

rm(ccs) # remove the uneccessary, duplicate object
```

